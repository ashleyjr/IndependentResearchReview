\documentclass[journal]{IEEEtran}
\usepackage{cite}

\usepackage{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{subfigure}
\usepackage{mathrsfs,amsmath,amsfonts}   %The amsmath package is included for \xrightarrow
\usepackage{listings}

\hypersetup{
	colorlinks=false,
	pdfborder={0 0 0},
}

\lstset{ %
  %backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  %commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,                    % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  %keywordstyle=\color{black},       % keyword style
  language=C,                 % the language of the code
  morekeywords={*,...},            % if you want to add more keywords to the set
  numbers=none,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  %numberstyle=\tiny\color{black}, % the style that is used for the line-numbers
  %rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  %stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
  %stringstyle=\color{black},     % string literal style
  tabsize=4,                       % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}





\begin{document}
\title{Tumour Detection By Volumetric Image Analysis}
\author{
	\IEEEauthorblockN{Ashley J. Robinson\\}
	\IEEEauthorblockA{	University of Southampton\\
	\href{mailto:ajr2g10@ecs.soton.ac.uk}{ajr2g10@ecs.soton.ac.uk}
	}	
}


% The paper headers
\markboth{COMP6033: Independent Research Review. $8$/$05$/$14$.}%	DRAFT REPORT.}%
{}

\maketitle





\begin{abstract}

This research review covers techniques available for detecting tumours in the human body with respect to practical implications in image processing and machine learning.   
An architecture is proposed in order to give direction to research and to allow consideration of stage interaction.
Normal conventions are followed but as targeted towards medical review a sub-module is included that provides confidence information for a domain expert.  
Volumetric image analysis techniques for pre-processing and feature extraction are reviewed followed by methods for feature selection and classification. 
 

In conclusion this review recommends methods for each processing stage starting with anisotropic diffusion, for edge preservation smoothing, followed by edge, curvature and shape extraction.
This leads to principal component analysis for dimensionality reduction then finally a support vector machine for classification.
AdaBoost is considered for improving classifier performance and further pruning the input space.
Human supervised deployment is considered, after extensive testing, whether stand-alone or as an addition to existing medical review software.

\end{abstract}







%\begin{IEEEkeywords}
%Tumour, Classification
%\end{IEEEkeywords}



\IEEEpeerreviewmaketitle







\section{Introduction}
\IEEEPARstart{T}{his} research review covers the use of volumetric image analysis in medicine to accurately detect the presence of tumours in the human body. 
The problem is to be approached by considering the general cases therefore concentration is not set on a specific area of the body.
Rather than trying to classify lung or brain tumours individually the goal is to consider what features tumours have in common.
Given an entire image of a human body is it possible to classify the presence of a tumour in any given section whilst using the same approach?
The motivation behind this is to make the most of medical scanning.
Dosages of radiation, cost and capture time are all reasons to reduce the number of required patient scans.
Thoroughly inspecting any data gathered is a constructive method of scan reduction.
A practical end objective is used to provide direction to research and maximise the resultant impact.
This is to provide an implementation recommendation for a tool that can assist with medical practitioner diagnosis.  
Available literature often considers 3D techniques as a simple extension of 2D implementations but in practice this is not the case and for this reason an emphasis is placed on locating research in practical volumetric analysis with good results.   


The architecture in Fig.~\ref{fig:Proposed} is the end recommendation framework and is inspired by similar topologies discussed in the literature~\cite{ahmed2011efficacy,kumar2011classification,hau07feat,sachdeva2011multiclass,kostis03three}.
A chain of stages leads from raw data gathered from the patient to a classification algorithm resulting in diagnosis.
A confidence sub-block enables an expert to view the performance of the entire system without knowing the intricacies of operation which means processing is required for domain translation.
In machine learning for medical applications a classification threshold has to be selected carefully as false negatives are more serious than false positives.

Raw data is captured from the patient and passed onwards for pre-processing then feature extraction.
Input patterns are produced dependent on a pre-determined list of known useful tumour features. 
Dimensionality reduction is then used to frame the data as such to make the most of the classification algorithm.
Binary classification, after hard thresholding, indicts the presence or absence of a tumour in the provided image.
Detection will not be triggered multiple times for images containing more than a single tumour. 
Confidence information at the very least will be a percentage proportional to the strength of detection therefore multiple tumours will produce a stronger response. 

\begin{figure}[!htb]
   \centering
   \includegraphics[width = 0.45\textwidth]{Figures/Proposed.pdf}
   \caption{Proposed system architecture.}
   \label{fig:Proposed}
\end{figure}











\section{Image Acquisition}
\label{sec:image}

A standard medical X-ray is tuned to produce an image of human bone.
The rays are attenuated by different materials in the body, more so by dense regions, and the image is delivered from this attenuation effect~\cite{kayvan2006biomedical}. 
This uses a single source and substrate pair to capture the image.
Taking multiple images from many different angles gathers more information and a 3D image can be made by stitching these together.
Fig.~\ref{fig:ct} describes the physical process of gathering data to be used in Computed Tomography (CT).
A ring allows images to be captured across a single plane then by moving in the orthogonal plane it is possible to compute a full volumetric image. 

\begin{figure}[!htb]
   \centering
   \includegraphics[width = 0.3\textwidth]{Figures/CT.pdf}
   \caption{Acquiring data from Computed Tomography~\cite{kayvan2006biomedical}.}
   \label{fig:ct}
\end{figure}

CT can build a 3D image from any method of non-invasive 2D image capture.
Different methods have trade-offs such as radiation dosages and resolution.
Ultrasound is a method of image acquisition which does not use electromagnetic radiation and requires little computational processing. 
This means internal organs can be viewed in real-time which is beneficial because it allows practitioners to view internal physical movement. 
It can be used for CT but higher resolution methods are usually favoured such as X-ray and Magnetic Resonance Imaging (MRI).

There are other non-intrusive methods that do not use electromagnetic or ultrasonic waves.
Hand palpitation is a common technique used by medical practitioners to gain an impression of abnormalities near the surface of the human body. 
Electromechanical apparatus employing the same technique can be used to transfer the geometry of a growth to a volumetric image~\cite{liu09haptic,wellman1997modeling}.  
Only part of the area under inspection is visible so the technique has obvious drawbacks.

Only high resolution grayscale sources will be considered in this review.
This data is typically stored as a series of 2D images which are scan slices.
An image viewer will build 3D images as and when required.
The common medical standard for these images is DICOM which holds the slices, relative position and also the patient's details~\cite{dicom11nema}.
Viewing an image naturally proves problematic because it is 3D data shown in an isometric fashion on a 2D screen.
Software provides interactive rendering, as shown in Fig.~\ref{fig:3d}, which allows the user to move three planes to gain an impression of a region.

\begin{figure}[!htb]
   \centering
   \includegraphics[width = 0.3\textwidth]{Figures/3Dview.png}
   \caption{An MRI scan of a healthy brain taken rendered by image review software~\cite{cia,slicer}.}
   \label{fig:3d}
\end{figure}












\section{Tumour Features}
\label{sec:tumour}

All tumours that can occur in the human body are caused by abnormalities in cell growth~\cite{cooper1992cancer}.
Tumours are dense areas of cells and can be either benign or malignant.
Malignant tumours are considered cancers because they invade nearby normal tissue and spread therefore making them an area of concern.
These are broken down in to three categories.
\emph{Carcinomas}, cells lining the body and internal organs, account for $90$\% of human cancers.
\emph{Leukemia} and \emph{Lymphomas}, occurring in the blood and lymph systems, account for $8$\% in humans.
\emph{Sarcomas} are solid regions of connective tissue which are rare.

All types of tumours are areas for concern.
Fig.~\ref{fig:growing} shows an initial accelerated growth rate producing a benign tumour which eventually becomes malignant as it spreads to nearby tissue.
Benign tumours and carcinomas in situ can be easily removed by surgery.
Once a tumour begins to metastasize surgery becomes ineffective. 
Detection should be tuned towards the dense regions of cells as shown in Fig.~\ref{fig:grow2} to allow detection before spreading and when spread locally so surgery remains a viable solution.    

\begin{figure}[!htb]
	\centering
	\subfigure[Hyperproliferation cell population.]{\includegraphics[width=0.12\textwidth,height=0.12\textwidth]{Figures/Stage1.pdf}
	\label{fig:grow1}}
	\subfigure[Small adenoma (benign).]{\includegraphics[width=0.12\textwidth,height=0.12\textwidth]{Figures/Stage2.pdf}
	\label{fig:grow2}}
	\subfigure[Large adenoma plus carcinoma (malignant).]{\includegraphics[width=0.12\textwidth,height=0.12\textwidth]{Figures/Stage3.pdf}
	\label{fig:grow3}}
\caption{Development of colon/rectum carcinomas~\cite{cooper1992cancer}.}
\label{fig:growing}
\end{figure}





Whether the tumour lines the host or grows independently the key to identification is to differentiate normal and abnormal cell density. 
Fig.~\ref{fig:ex} contains examples of identified tumours in different organs.
Tumours shown in the brain and lungs, Fig.~\ref{fig:exBrain} and Fig.~\ref{fig:exLung} respectively, are far simpler to identify than the liver example image in Fig.~\ref{fig:exLiver}.
An image of a liver will contain other organs so these must be considered when trying to extract features.

\begin{figure}[!htb]
	\centering
	\subfigure[Brain~\cite{islam12seg}]{\includegraphics[width=0.12\textwidth,height=0.12\textwidth]{Figures/Brain.pdf}
	\label{fig:exBrain}}
	\subfigure[Lung~\cite{amutha13lung}]{\includegraphics[width=0.12\textwidth,height=0.12\textwidth]{Figures/Lungs.pdf}
	\label{fig:exLung}}
	\subfigure[Liver~\cite{chen07mr}]{\includegraphics[width=0.12\textwidth,height=0.12\textwidth]{Figures/Liver.pdf}
	\label{fig:exLiver}}
\caption{Example MRI images containing tumours.}
\label{fig:ex}
\end{figure}











\section{Pre-processing}
\label{sec:pre}

The analogue of a pixel (picture element) in a 2D image is called a voxel (volumetric element) in a 3D image which uses an extra dimension to form a cube~\cite{lohmann1998volumetric}.
They can be considered cubes but also points which may make visualisation and processing easier to understand.
Fig.~\ref{fig:neighbourhood} describes how voxels are packed and the three different neighbourhoods that can be considered when inspecting a single voxel. 
Considering voxels to have larger neighbourhoods can increase the time complexity of some algorithms.


\begin{figure}[!htb]
	\centering
	\subfigure[6]{\includegraphics[width = 0.12\textwidth]{Figures/6N.pdf}
	\label{fig:neighbourhood6}}
	\subfigure[18]{\includegraphics[width = 0.12\textwidth]{Figures/18N.pdf}
	\label{fig:neighbourhood18}}
	\subfigure[26]{\includegraphics[width = 0.12\textwidth]{Figures/26N.pdf}
	\label{fig:neighbourhood26}}
\caption{Voxel Neighbourhoods~\cite{lohmann1998volumetric}.}
\label{fig:neighbourhood}
\end{figure}


The raw data provided needs to be converted from the delivery format of the acquisition technology to a raw 3D matrix ready for generic processing.
Watermarking, which may have been inserted for human review, should also be removed at this stage. 
This is trivial in theory but in practice requires a database of artefacts or manual review to accomplish.
There are also techniques, not considered in this report, which can be applied only to the slices of volumetric images therefore reducing computation overheads~\cite{harauz86exact}.

Pre-processing methods are used to enhance the image captured and ensure it is correctly formatted for further processing.
This is considered as an image mapping, $\textbf{I} \mapsto \textbf{I}'$, using only information from the original image.

\subsection{Histogram Operations}

In this context a histogram is a mapping from the image to cumulative voxel intensity.
This is possible because voxels only have discrete values of intensity within a fixed range.
Some materials can be determined from image intensity therefore a histogram contains information on the quantity of a given material contained in the image.
Image enhancement can be performed using point operations with information from the histogram.
This is a fast method of enhancement as it is a direct voxel-to-voxel mapping as shown in Eq.~\eqref{eqn:point}. 
Normalisation stretches the histogram to cover all available values of intensity therefore improving contrast.
Equalisation attempts to flatten the histogram by distributing intensity~\cite{nixon02feature}.


\begin{equation}
	I'(u,v,w) = \alpha(u,v,w)I(u,v,w)
	\label{eqn:point} 
\end{equation}



\subsection{Spatial Filtering}
\label{sec:spatial}
Spatial filters use the neighbourhood of voxels to create a new value for each voxel in an image. 
The original image is mapped through a filter kernel as described in Eq.~\eqref{eqn:kernel}.
The filter kernel is a fixed dimension sub-image with a weighting on each voxel that is applied to every voxel in the original image.
This is spatial convolution.
When all elements of the filter kernel are unity it is known as a \emph{box filter} and performs smoothing on the image to remove noise.
A Gaussian filter performs better by placing a higher weighting on the central voxel and less weighting on the edge voxels inside the kernel~\cite{lohmann1998volumetric}.
The edge voxels of an image do not have a complete set of neighbours therefore performing spatial filtering will reduce the total size of the image in further processing.

\begin{equation}
	I'(u,v,w) = \sum\limits_{(i,j,k) \in R_H} I(u + i,v + j, w + k)H(i,j,k)
	\label{eqn:kernel} 
\end{equation}



\subsection{Frequency Filtering}
The Fourier transform, as used spatially on 2D images, can be extended to 3D images.
A Discrete Fourier Transform (DFT) for an $N^3$ voxel image is held in Eq.~\eqref{eqn:dft}.
The spatial filtering process performs convolution so once an image is in the frequency domain the same effects can be achieved using multiplication.
It may prove more efficient to filter an image in the frequency domain rather than the spatial domain if the required filter kernel is large.
Band-pass and notch filter operations can also be performed in the frequency domain.
Eq.~\eqref{eqn:same} provides the same result as Eq.~\eqref{eqn:kernel} where $\mathcal{F}$ is the transform described by Eq.~\eqref{eqn:dft}.

\begin{equation}
	F_{u,v,w} = \frac{1}{N^{\frac{3}{2}}} \sum\limits_{x=-\frac{N}{2}}^{\frac{N}{2}-1}\sum\limits_{y=-\frac{N}{2}}^{\frac{N}{2}-1}\sum\limits_{z=-\frac{N}{2}}^{\frac{N}{2}-1}I_{x,y,z}e^{-j\frac{2\pi}{N}(ux + vy + wz)}
	\label{eqn:dft} 
\end{equation}

%\begin{equation}
%	\textbf{F} = \mathcal{F}(\textbf{P}),\:\:\textbf{P} = \mathcal{F}^{-1}(\textbf{F}) 
%	\label{eqn:ft} 
%\end{equation}

\begin{equation}
	\textbf{I}' = \mathcal{F}^{-1}(\mathcal{F}(\textbf{I})\times\mathcal{F}(\textbf{H})))		% Arrow doesn't work, implied matrices?
	\label{eqn:same} 
\end{equation}


\subsection{Anisotropic Diffusion}
Anisotropic diffusion is a powerful technique used to enhance images where it is possible to remove noise yet retain clear edges.
The technique has been expanded from common 2D applications to 3D medical usage on MRI data~\cite{nakh11three}.
The diffusion effect has the same outcome as a low pass filter with reasonably adjusted values for resolution, $\Delta t$, and the decay constant, $\mathbf{k}$.
The spatial scalar decay constant is a function of the image and time which must preserve edges by setting $k(u_{edge},v_{egde},w_{edge},t) = 0$.
Using Eq.~\eqref{eqn:anisotropic} an iterative approach is provided in Eq.~\eqref{eqn:anisotropic_it} where the original image is the value of $\mathbf{I}$ at time zero.
Iteration monitoring and reasonable scalar values in the function $\mathbf{k}$ are required to produce a good outcome.
Edge detection methods to be used for $\mathbf{k}$ are discussed in section~\ref{sec:edge}.

\begin{equation}
	\frac{\partial \mathbf{I}}{\partial t} = \mathbf{k(\mathbf{I})} \mathbf{\nabla}^2 \mathbf{I},\:\:\mathbf{k},\mathbf{I} \in \mathbb{R}^4
	\label{eqn:anisotropic} 
\end{equation}

\begin{equation}
	\mathbf{I}_{t + \Delta t} = \mathbf{I}_t + \Delta t\mathbf{k}(\mathbf{I}_t)\mathbf{\nabla}^2\mathbf{I}_t
	\label{eqn:anisotropic_it} 
\end{equation}















\section{Feature Extraction}
\label{sec:extraction}

High level feature extraction of volumetric shapes should be used on the input image.
Extracted regions may include organ walls or bones but should also include potential tumours.


\subsection{Edge Detection}
\label{sec:edge}
Edge detection can be performed using convolution with a kernel template as discussed in section~\ref{sec:spatial}. 
These templates are designed to create a new image containing voxel differences and can therefore be thought of as taking the derivative of the image.
2D templates are common and well tested~\cite{nixon02feature}.
As all dimensions must be considered during differencing the 3D application is extended in the third dimension as shown in Fig.~\ref{fig:sobel}.
Smaller filter masks are also available to reduce computation but with diminished results.

\begin{figure}[!htb]
   \centering
   \includegraphics[width = 0.2\textwidth]{Figures/Sobel.pdf}
   \caption{3D Sobel filter mask~\cite{lohmann1998volumetric}.}
   \label{fig:sobel}
\end{figure}

In edge critical processing better results can be produced with Canny edge detection~\cite{canny86edge}. 
This is a four stage process that uses Gaussian smoothing, the Sobel operator, non-maximal suppression followed by hysteresis thresholding.
Advantages include reduced noise, minimised position difference in the mapping and elimination of multiple responses to an edge.




\subsection{Curvature Detection}
\label{sec:curvature_detection}
The output of an edge detection method can be measured with respect to curvature.
Tumours can extrude from flat surfaces of organ walls therefore measuring curvature will be useful for classification.
Curvature can be expressed as the rate of change in the normal along a line, $g(t)$, which is described by Eq.~\ref{eqn:curv}.
Surface measurements, required in volumetric detection, contain infinitely many curves at a given point~\cite{lohmann1998volumetric}.
The principle directions, those of most and least curvature, are evaluated then by taking the mean and Gaussian values, $H = (k_{max} + k_{min})/2$ and $K = k_{max}k_{min}$, so that the category of curvature at a point can be detected.
On a plane $H = 0,\;K=0$, on a peak $H<0,\;K>0$ and at a saddle point $K<0$.
An absolute value of curvature can be taken as $E_c=|H|+|K|$.

\begin{equation}
	k = \frac{g''(t)}{(1 + g'(t)^2)^{\frac{3}{2}}}
	\label{eqn:curv}
\end{equation}


\subsection{Hough Transform (HT)}

The HT uses a dual space as an accumulator for evidence based location of a shape inside an image~\cite{nixon02feature}.
It has been successfully applied to 3D images to locate spherical shapes~\cite{abuzaina13hough}.
Some tumours grow spherically, brain tumours for example in Fig.~\ref{fig:exBrain}, and the ability to fit a sphere is a useful feature for classification.
The distribution of the accumulator space can be used to indicate how many potential spheres fit the image but this is expected to be noisy as it is unlikely tumours will be exact spheres.





\subsection{Active Surface Contours}

The active contour model is a method for flexible shape extraction and has been thoroughly trialled in 2D~\cite{nixon02feature}.
Volumetric implementations are state of the art and have been successfully applied to prostate, nerve fibre and artery segmentation~\cite{skalski13automatic,muralidhar12active,xiao12carotid}.
These are not pure active contours and emulate behave by segmenting 2D regions then using volumetric growth algorithms to actively converge upon a 3D shape.
This works for certain parts of human anatomy but typically known geometry is required to influence the growing algorithm. 

Pure active contours could improve performance and using current research it is possible to expand dimensionality.
The surface is defined as $\mathbf{v}(r,s) = [x(r,s),y(r,s),z(r,s)]$ where $r,s \in [0,1]$.
Surface energy is defined as Eq.~\eqref{eqn:contour_v} where the objective is minimisation of internal, external and constraint functions with respect to the entire surface~\cite{nixon02feature,skalski13automatic}. 
The internal energy is measured using the first and second order differentials of the surface contour.
Eq.~\eqref{eqn:contour_int} describes internal energy with two parametric functions chosen prior to fitting.
These control elasticity, $\alpha$, and stiffness, $\beta$, of the contour.
The external energy, Eq.~\ref{eqn:contour_ext}, is effected by three variables.
Image intensity is controlled by the sign of $w_l$ which attracts the contour to light or dark voxels.
Edge detection controlled by $w_e$ and curvature used for termination controlled by $w_t$.
Constraint energy is imposed by high level design, for example this could be tuned to favour a certain size of contour.


\begin{equation}
	E = \int_0^{1}\int_0^{1} E_{int}(\mathbf{v}) + E_{ext}(\mathbf{v}) + E_{con}(\mathbf{v})\:dr\;ds
	\label{eqn:contour_v}
\end{equation}

\begin{equation}
	 E_{int}= \alpha(r,s)\left|\nabla \mathbf{v}\right|^2 + \beta(r,s)\left|\nabla^2 \mathbf{v}\right|^2 
	 \label{eqn:contour_int}
\end{equation}

\begin{equation}
	 E_{ext}= w_{l}\mathbf{I} + w_{e}|\nabla \mathbf{I}|^2 + w_{t}E_c(\mathbf{I})
	 \label{eqn:contour_ext}
\end{equation}















\section{Feature Selection}
\label{sec:selection}

Commonly large datasets contain some features in the input space which will have no effect upon or even reduce the performance of a classifier.
It is naive to use just the raw image data.
Tumour features for extraction have been considered in section~\ref{sec:tumour} which already removes some redundant information. 
Mapping input data into a lower dimensional space is possible and can improve performance but if ill-applied can remove key features.
A brute-force approach would find the optimal combination of features but the input space is expected to be too large, $2^n$ is a challenge even moderate values of $n$, to be completely shattered within reasonable execution time. 



\subsection{Sequential Floating Search (SFS)}

This heuristic algorithm uses feature subsets and has been applied to tumour classification~\cite{hau07feat}.
It is possible to start from either the full or empty set of features then iteratively add and remove features to improve the performance of the classifier. 
A performance function is required for feature set comparison. 
Listing~\ref{lst:sfs} contains the algorithm starting from an initial subset of no features and repeated until convergence.

\lstinputlisting[label=lst:sfs,caption=SFS Algorithm.]{Code/sfs.txt}


\subsection{Stochastic Approaches}
A Random Mutation Hill Climber (RMHC) can move between in the combinatorial feature selection space from an initial starting point but will never reduce in performance.  
This does not handle a landscape with many local maxima well and may not produce a reasonable solution.
Genetic Algorithms (GAs) are more computationally expensive as they require a population of solutions using crossover and mutation to converge on a suitable solution.   
Whether it out performs a RMHC depends on the landscape but using a population of solutions is certainly more thorough.   
Applied to MR images of the brain a GA yielded better results than feature subset algorithms~\cite{sasikala05ga}.
Simulated annealing uses mutation to move between neighbouring solutions over a fixed number of iterations.
A cooling schedule is used to except negative moves in performance with a probability that is a function of the cooling variable. 


\subsection{Principle Component Analysis (PCA)}
PCA is a common technique used for dimensionality reduction by ranking dimensions in order of variance~\cite{barberBRML2012}.
This will rank the features so that only the first $L$ are passed on for further processing. 
The other dimensions can simply be removed but if the effect of a low variance dimension is important to classification this can reduce performance.
Eq.~\eqref{eqn:PCA} shows how the original input pattern is mapped to a reduce pattern by subtracting the mean of all patterns and multiplying with the projection matrix.
The projection matrix is built by finding the covariance matrix of the input patterns then ranking the eigenvectors by the eigenvalues and deleting as required.
Performance feedback from training can be used to select appropriate value for $L$.
PCA has been successfully coupled with tumour detection~\cite{ahmed2011efficacy}.

\begin{equation}
	\boldsymbol{z} = \boldsymbol{P}(\boldsymbol{x} - \boldsymbol{\mu})
	\label{eqn:PCA}
\end{equation}












\section{Classification}
\label{sec:class}

Classification must differentiate between extracted geometric shapes that are expected and those which may potentially be tumours.
Tumours will also be attached to normal internal body parts so the classifier must be able to handle any abnormalities in organs.
Supervised techniques for classification are considered even though datasets have not yet been sourced.



\subsection{Artificial Neutral Networks (ANNs)}
ANNs contain summation nodes where data is propagated through weighted connections towards an output. 
As a supervised technique weights are trained by propagating the error backwards through the network.
Multilayer nets can provide non-linear separation functions and increasing the number of layers only increases possible function complexity.
Neural networks have been applied successfully to classification of brain tumours~\cite{amin12brain}.

Eq.~\eqref{eqn:mlp} is an example of a two layer network with an $n$ dimensional input space and $m$ hidden nodes.
A $tanh$ activation function is used on the output of the hidden nodes but only the sign is used for the binary classification task.
There is also a scalar offset, $b$, on the output and an offset vector, $\mathbf{c}$, on the hidden layer. 

\begin{equation}
	y = sign \left(b + \sum_{i=1}^{m} u_i \tanh \left(c_i + \sum_{j=1}^{n} x_j w_{j,i}\right)  \right) 
	\label{eqn:mlp}
\end{equation}



\subsection{Support Vector Machines (SVMs)}
An SVM classifier builds a separation margin supported by the closest data points then draws a separation plane from the middle of this margin.
A linear maximum-margin hyperplane is produced but non-linear data can also be separated by using the kernel trick~\cite{cortes95support}. 
This is a mapping of the input space into a higher dimensional space so that it becomes linearly separable.
SVMs have been successfully applied to breast cancer diagnosis~\cite{xiufeng13svm}.
The approach reviewed uses a weighted combination of polynomial and radial basis functions to improve the generalisation performance.



\subsection{Boosting}
A weak learner for binary classification can be considered any classifier with an error rate less than $50$\%.
It is possible to create a strong hypothesis, having a much smaller error rate, by combing the weighted sum of different weak hypothesises provided by a weak learner. 
The algorithm can be considered a hybrid of feature selection and classification.

AdaBoost is the most common implementation of a boosting algorithm and has been successfully applied to brain tumour classification~\cite{freund99boost,islam13multi}.
The architecture of AdaBoost can be thought of as single layer ANN but trained using the error in each weak hypothesis with an input vector length equivalent to the number of boosting rounds.
The weights for each hypothesis is calculated using Eq.~\eqref{eqn:adaboost} where $\epsilon_t$ is the error which means anything greater than $0.5$ is negative and as $\epsilon_t \to 0$ then $\alpha_t \to \infty$ meaning a perfect classifier would dominate.
The distribution of data to train the weak learner for the next hypothesis is also determined using Eq.~\eqref{eqn:adaboost} therefore contributing to feature selection.


\begin{equation}
	\alpha_t = \frac{1}{2}\ln\left(\frac{1 - \epsilon_t}{\epsilon_t}\right)
	\label{eqn:adaboost}
\end{equation}






\section{Conclusions}
\label{sec:conclusions}

It is clear that this tool cannot be used as a hard and fast method of tumour diagnosis but instead as a valuable tool for image reviewers.
Software packages designed for medical image review have bidirectional interfaces to custom extensions~\cite{slicer}.
Implementing this system as an extension would remove some overheads required for data handling but reduce the scope for optimisation.
The feedback from the entire process would be a checklist which the user must manually review and waive if incorrectly identified.

Medical images are high resolution but noise can still be an issue and a pre-processing stage is reasonable.
The acquisition method should be verified to contain no image processing as this could produce redundancy.
Anisotropic diffusion is recommended to reduce noise because edge detection is important in later stages.
This method can also by easily tuned by limiting iterations of the algorithm to match the image source.

Edge detection is for required for both pre-processing and feature extraction.
Canny edge detection produces significantly better results than simple filter kernels but at the expense of computation.
Edge, curvature and active contour feature extraction techniques are recommended for implementation.
A HT would provide a large amount of noisy data which would be detrimental to later stages.
To deal with amount of features PCA can be used to remove redundant components but also AdaBoost in conjunction with the classifier algorithm.
SVMs are robust machine learning architectures and should defiantly provide at least weak hypothesises which should prove useful in the boosting algorithm. 


The confidence analysis unit will take noise information from the initial image by differencing the input and output of the pre-processing stage.
In feature extraction it can report on the quantity of shape extraction from an image. 
Dimensionality reduction will have no output as the reduction value is tuned prior to deployment.
The binary classifier also has a non-discrete output before hard thresholding which can be used as a measure of confidence.
This can also be used to adaptively control the number of boosting rounds.


This research review has provided a recommendation for population of a conventional image processing architecture ending with standard a machine learning approach.
Volumetric techniques for pre-processing and feature extraction have been reviewed extensively. 
Cutting edge implementations have been considered for anisotropic diffusion along with active surface contours which have also been expanded to the edge of current research.


\bibliographystyle{ieeetran}
\bibliography{references}

\end{document}


