\documentclass[journal]{IEEEtran}
\usepackage{cite}

\usepackage{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{subfigure}
\usepackage{mathrsfs,amsmath,amsfonts}   %The amsmath package is included for \xrightarrow
\usepackage{listings}

\hypersetup{
	colorlinks=false,
	pdfborder={0 0 0},
}

\lstset{ %
  %backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  %commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,                    % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  %keywordstyle=\color{black},       % keyword style
  language=C,                 % the language of the code
  morekeywords={*,...},            % if you want to add more keywords to the set
  numbers=none,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  %numberstyle=\tiny\color{black}, % the style that is used for the line-numbers
  %rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  %stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
  %stringstyle=\color{black},     % string literal style
  tabsize=4,                       % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}


\begin{document}
\title{Tumour Classification By Volumetric Image Analysis}
\author{
	\IEEEauthorblockN{Ashley J. Robinson\\}
	\IEEEauthorblockA{	University of Southampton\\
	\href{mailto:ajr2g10@ecs.soton.ac.uk}{ajr2g10@ecs.soton.ac.uk}
	}	
}


% The paper headers
\markboth{COMP6033: Independent Research Review. $29$/$04$/$14$.}%	DRAFT REPORT.}%
{}

\maketitle


\begin{abstract}

This research review covers techniques available for binary classification of tumours in the human body considering practical implications of individual methods.   
An architecture is proposed in order to give direction to research and to allow consideration of stage interactions.
Targeted at medical images the system follows standard image processing conventions and contains a sub-module that provides confidence information for a domain expert.  
Only volumetric images processing is considered.
Pre-processing and Feature extraction techniques are reviewed. 
Machine learning techniques for feature selection and classification are also considered. 

The recommendation for population begins with anisotropic diffusion, for edge preservation smoothing, followed by edge detection and shape extraction.
Sequential floating search for feature selection then finally a support vector machine for classification.
Human supervised deployment is considered whether stand-alone or as an extension to existing medical review software.

\end{abstract}







%\begin{IEEEkeywords}
%Tumour, Classification
%\end{IEEEkeywords}



\IEEEpeerreviewmaketitle







\section{Introduction}
\IEEEPARstart{T}{his} research review covers the use of volumetric image analysis in medicine to accurately classify the presence of tumours in the human body. 
The problem is to be approached by considering the general cases therefore concentration is not set on a specific area of the human body.
Rather than trying to classify lung or brain tumours individually the goal is to consider what features tumours have in common and given an entire image of a human body is it possible to classify tumours in any given section?
The motivation behind this is to make the most of medical scanning.
Dosages of radiation, cost and capture time are all reasons to reduce the number of required patient scans.
Throughly inspecting any data gathered is a constructive method of scan reduction.
A practical end objective is used to provide direction to research and maximise the resultant impact.
The is to provide a recommendation for a tool to be implemented to assist in medical practitioner diagnosis.  

The architecture in Fig.~\ref{fig:Proposed} is the framework of the system to be recommended and is inspired by those discussed in~\cite{ahmed2011efficacy,kumar2011classification,hau07feat,sachdeva2011multiclass,kostis03three}.
A chain of stages leads from raw data gathered from the patient to a diagnosis from a classification algorithm.
A confidence sub-block enables an expert to view the performance of the entire system without knowing the intricacies of operation which means processing is required for domain translation.
In machine learning for medical applications a classification threshold has to be selected carefully as false negatives are more serious the false positive.

Raw data is captured from the patient and passed to the system for pre-processing then feature extraction.
Input patterns are produced dependent on a pre-determined list of known useful tumour features. 
Dimensionality reduction is then used to frame the data as such to make the most of the classification algorithm.

\begin{figure}[!htb]
   \centering
   \includegraphics[width = 0.4\textwidth]{Figures/Proposed.pdf}
   \caption{Proposed system architecture.}
   \label{fig:Proposed}
\end{figure}











\section{Image Acquisition}
\label{sec:image}

A standard medical X-ray is tuned to produce an image of human bone.
The rays are attenuated by different materials in the body, more so by dense regions, and the image is gained from this attenuation effect~\cite{kayvan2006biomedical}. 
This uses a single source and substrate pair to capture the image.
Taking multiple images from many different angles gathers more information and a 3D image can be made by stitching these together.
Fig.~\ref{fig:ct} describes the physical process of gathering data to be used in Computed Tomography (CT).
A ring allows images to be capture from 360$^{\circ}$ in a single plane then by moving in the orthogonal plane, in Fig.~\ref{fig:ct} through the paper, a full volumetric image of the patient can be made. 

\begin{figure}[!htb]
   \centering
   \includegraphics[width = 0.3\textwidth]{Figures/CT.pdf}
   \caption{Acquiring data from Computed Tomography. Adapted from~\cite{kayvan2006biomedical}.}
   \label{fig:ct}
\end{figure}

Any method of non-invasive image capturing in 2D can therefore be expanded to work in 3D by using the CT principle.
Different methods have trade-offs such as radiation dosages and resolution.
Ultrasound is a method of image acquisition which does not use electromagnetic radiation and requires little computational processing. 
This means internal organs can be viewed in real-time which is beneficial because it allows practitioners to view internal physical movement. 
It can be used for CT but higher resolution methods are usually favoured such as X-ray and Magnetic Resonance Imaging (MRI).

There are also other non-intrusive methods that do not use electromagnetic or ultrasonic waves.
Hand palpitation is a common technique used by medical practitioners to gain an impression of abnormalities near the surface of the human body. 
Electromechanical apparatus employing the same technique can be used to transfer the geometry of a growth to a volumetric image~\cite{liu09haptic,wellman1997modeling}.  
Only part of the area under inspection is visible so the technique has obvious drawbacks.

Only high resolution grayscale sources will be considered in this review.
This data is typically stored as a series of 2D images which are scan slices.
An image viewer will build 3D images as and when required.
The common medical standard for these images is DICOM which holds the slices, relative position and also the patient's details~\cite{dicom11nema}.
Viewing an image naturally proves problematic because it is 3D data shown in an isometric fashion on a 2D screen.
Software provides interactive images as shown in Fig.~\ref{fig:3d} which allows the user to move three planes to gain an impression of a region.

\begin{figure}[!htb]
   \centering
   \includegraphics[width = 0.3\textwidth]{Figures/3Dview.png}
   \caption{An MRI scan of a healthy brain taken from~\cite{cia} and rendered by~\cite{slicer}.}
   \label{fig:3d}
\end{figure}












\section{Tumour Features}
\label{sec:tumour}

There are hundreds of types of tumours that can occur in the human body but they are all caused by abnormalities in cell growth~\cite{cooper1992cancer}.
Tumours are dense areas of cells and can be either benign or malignant.
Malignant tumours are considered cancers because they invade nearby normal tissue and spread therefore making them an area of concern.
These are broken down in to three categories.
\emph{Carcinomas}, cells lining the body and internal organs, account for $90$\% of human caners.
\emph{Leukemias} and \emph{Lymphomas}, occurring in the blood and lymph systems, account for $8$\% in humans.
\emph{Sarcomas} are solid regions of connective tissue which are rare.


Whether the tumour lines the host or grows independently the key to identification is to differentiate normal and abnormal cell density. 
Fig.~\ref{fig:ex} contains examples of identified tumours in different organs.
Tumours shown in the brain (Fig. \ref{fig:exBrain}) and lung (Fig. \ref{fig:exLiver}) are far simpler to identify than the liver example image (Fig. \ref{fig:exLiver}).
An image of a liver will contain other organs so these must be considered when trying to extract features.

\begin{figure}
	\centering
	\subfigure[Brain~\cite{islam12seg}]{\includegraphics[width=0.12\textwidth,height=0.12\textwidth]{Figures/Brain.pdf}
	\label{fig:exBrain}}
	\subfigure[Lung~\cite{amutha13lung}]{\includegraphics[width=0.12\textwidth,height=0.12\textwidth]{Figures/Lungs.pdf}
	\label{fig:exLung}}
	\subfigure[Liver~\cite{chen07mr}]{\includegraphics[width=0.12\textwidth,height=0.12\textwidth]{Figures/Liver.pdf}
	\label{fig:exLiver}}
\caption{Example MRI images containing tumours.}
\label{fig:ex}
\end{figure}











\section{Pre-processing}
\label{sec:pre}

The analogue of a pixel (picture element) in a 2D image is called a voxel (volumetric element) in a 3D image.
This is exactly the same idea but with a extra dimension to form a cube~\cite{lohmann1998volumetric}.
They can be considered cubes but also points which may make visualisation and processing easier to understand.
Fig.~\ref{fig:neighbourhood} describes how voxels are packed and the three different neighbourhoods that can be considered when inspecting a single voxel. 
Considering voxels to have larger neighbourhoods can increase the time complexity of some algorithms.


\begin{figure}
	\centering
	\subfigure[6]{\includegraphics[width = 0.12\textwidth]{Figures/6N.pdf}
	\label{fig:neighbourhood6}}
	\subfigure[18]{\includegraphics[width = 0.12\textwidth]{Figures/18N.pdf}
	\label{fig:neighbourhood18}}
	\subfigure[26]{\includegraphics[width = 0.12\textwidth]{Figures/26N.pdf}
	\label{fig:neighbourhood26}}
\caption{Voxel Neighbourhoods~\cite{lohmann1998volumetric}.}
\label{fig:neighbourhood}
\end{figure}


The raw data provided needs to be converted from the delivery format of the acquisition technology to a raw 3D matrix ready for generic processing.
Watermarking, which may have been inserted for human review, should also be removed at this stage. 
This is trivial in theory but requires a database of artefacts or manual review to accomplish.
There are also techniques, not considered in this report, which can be applied only to the slices of volumetric images therefore reducing computation overheads~\cite{harauz86exact}.

Pre-processing methods are used to enhance the image captured and ensure it is correctly formatted for further processing.
This is considered as an image mapping, $\textbf{I} \mapsto \textbf{I}'$, using only information from the original image.

\subsection{Histogram Operations}

In this context a histogram is a mapping from the image to cumulative voxel intensity.
This is possible because voxels only have discrete values of intensity within a fixed range.
Some materials can be determined from image intensity therefore a histogram contains information on the quantity of a given material contain in the image.
Image enhancement can be performed using point operations with information from the histogram.
This is a fast enhancement method as it is direct voxel-to-voxel mapping, as shown in Eq.~\eqref{eqn:point}. 
Normalisation stretches the histogram to cover all available values of intensity therefore improving contrast.
Equalisation attempts to flatten the histogram by distributing intensity~\cite{nixon02feature}.


\begin{equation}
	I'(u,v,w) = \alpha(u,v,w)I(u,v,w)
	\label{eqn:point} 
\end{equation}



\subsection{Spatial Filtering}
\label{sec:spatial}
Spatial filters use the neighbourhood of voxels to create a new value for each voxel in an image. 
The original image is mapped through a filter kernel as described in Eq.~\eqref{eqn:kernel}.
The filter kernel is a fixed dimension sub-image with a weighting on each voxel that is applied to every voxel in the original image.
This is spatial convolution.
If the filter kernel contains all ones it is known as a \emph{box filter} and performs smoothing on the image to remove noise.
A Gaussian filter performs better by placing a higher weighting on the central voxel and less weighting on the edge voxels inside the kernel~\cite{lohmann1998volumetric}.
The edge voxels of an image do not have a complete set of neighbours therefore performing spatial filtering will reduce the total size of the image in further processing.

\begin{equation}
	I'(u,v,w) = \sum\limits_{(i,j,k) \in R_H} I(u + i,v + j, w + k)H(i,j,k)
	\label{eqn:kernel} 
\end{equation}



\subsection{Frequency Filtering}
The Fourier transform, as used spatially on 2D images, can be extended to 3D images.
A Discrete Fourier Transform (DFT) for an $N^3$ voxel image is held in Eq.~\eqref{eqn:dft}.
The spatial filtering process performs convolution so once an image is in the frequency domain the same effects can be achieved using multiplication.
It may prove more efficient to filter an image in the frequency domain rather than the spatial domain if the required filter kernel is large.
Band-pass and notch filter operations can also be perform in the frequency domain.
Eq.~\eqref{eqn:same} provides the same result as Eq.~\eqref{eqn:kernel} where $\mathcal{F}$ is described by Eq.~\eqref{eqn:dft}.

\begin{equation}
	F_{u,v,w} = \frac{1}{N^{\frac{3}{2}}} \sum\limits_{x=-\frac{N}{2}}^{\frac{N}{2}-1}\sum\limits_{y=-\frac{N}{2}}^{\frac{N}{2}-1}\sum\limits_{z=-\frac{N}{2}}^{\frac{N}{2}-1}I_{x,y,z}e^{-j\frac{2\pi}{N}(ux + vy + wz)}
	\label{eqn:dft} 
\end{equation}

%\begin{equation}
%	\textbf{F} = \mathcal{F}(\textbf{P}),\:\:\textbf{P} = \mathcal{F}^{-1}(\textbf{F}) 
%	\label{eqn:ft} 
%\end{equation}

\begin{equation}
	\textbf{I}' = \mathcal{F}^{-1}(\mathcal{F}(\textbf{I})*\mathcal{F}(\textbf{H})) = \mathcal{F}^{-1}(\textbf{I}**\textbf{H})		% Arrow doesn't work, implied matrices?
	\label{eqn:same} 
\end{equation}


\subsection{Anisotropic Diffusion}
Anisotropic diffusion is a powerful technique used to enhance images where it is possible to remove noise yet retain edges.
The technique has been expanded from common 2D applications to 3D medical usage on MRI data~\cite{nakh11three}.
The diffusion effect has the same outcome as a low pass filter with reasonably adjusted values for $\Delta t$ and the decay constant.
The spatial scaler $\mathbf{k}$ is a function of the image and must preserve edges by setting $k(u_{edge},v_{egde},w_{edge},t) = 0$.
Using Eq.~\eqref{eqn:anisotropic} an iterative approach is provided in Eq.~\eqref{eqn:anisotropic_it} where the original image is the value of $\mathbf{I}$ at time zero.
Iteration monitoring and reasonable scalar values in the function $\mathbf{k}$ are required to produce a good outcome.
Edge detection methods to be used for $\mathbf{k}$ are discussed in section~\ref{sec:edge}.

\begin{equation}
	\frac{\partial \mathbf{I}}{\partial t} = \mathbf{k(\mathbf{I})} \mathbf{\nabla}^2 \mathbf{I},\:\:\mathbf{k},\mathbf{I} \in \mathbb{R}^4
	\label{eqn:anisotropic} 
\end{equation}

\begin{equation}
	\mathbf{I}_{t + \Delta t} = \mathbf{I}_t + \Delta t\mathbf{k}(\mathbf{I}_t)\mathbf{\nabla}^2\mathbf{I}_t
	\label{eqn:anisotropic_it} 
\end{equation}















\section{Feature Extraction}
\label{sec:extraction}

High level feature extraction of volumetric shapes should be used on the input image.
These high intensity regions may be organ walls or bones but should also include potential tumours.


\subsection{Edge Detection}
\label{sec:edge}
Edge detection can be performed using convolution with a kernel template as discussed in section~\ref{sec:spatial}. 
These templates are designed to create a new image contain voxel differences and can therefore be thought of as taking the derivative of the image.
2D templates are discussed in~\cite{nixon02feature} and templates are the same for volumetric images.
As all dimensions must be considered during differencing the 3D application is extended in the third dimension as shown in Fig~\ref{fig:sobel}.
Smaller filter masks are also available to reduce computation but with dimimnished results.

\begin{figure}[!htb]
   \centering
   \includegraphics[width = 0.2\textwidth]{Figures/Sobel.pdf}
   \caption{3D Sobel filter mask~\cite{lohmann1998volumetric}}
   \label{fig:sobel}
\end{figure}

In edge critical processing better results can be produced with Canny edge detection~\cite{canny86edge}. 
A four stage process that uses Gaussian smoothing, the Sobel operator, non-maximal suppression followed by hysteresis thresholding.
This reduces noise, minimises position difference in the mapping and eliminates multiple responses to an edge.





\subsection{Hough Transform (HT)}

The HT uses a dual space as an accumulator for evidence based location of a shape inside an image~\cite{nixon02feature}.
It has been successfully applied to 3D images to locate spherical shapes~\cite{abuzaina13hough}.
Some tumours, Fig.~\ref{fig:exBrain}, grow spherically so the ability to fit a sphere is a useful feature for classification.
The distribution of the accumulator space can be used to indicate how many potential spheres fit the image but this si expected to be noisy as it is unlikely tumours will be exact spheres.





\subsection{Active Contours}

The active contour model is a method for flexible shape extraction.
Expanding upon common 2D implementations active contours can be used to extract features from volumetric images.
Eq.~\eqref{eqn:contour_v} contains the standard energy minimisation function~\cite{nixon02feature,skalski13automatic}. 

\begin{equation}
	E = \int_0^{1} E_{int}(\mathbf{v}) + E_{ext}(\mathbf{v}) + E_{con}(\mathbf{v})\:ds,\:\:\mathbf{v}(s) \in \mathbb{R}^3
	\label{eqn:contour_v}
\end{equation}

The internal energy is measured using the first and second order differentials of the contours.
Eq.~\eqref{eqn:contour_int} contains internal energy with two parametric functions chosen prior to fitting.
These control elasticity, $\alpha$, and stiffness, $\beta$, of the contour.
The external energy is effected by three variables.
Image intensity is controlled by the sign of $w_l$ which attracts the contour to light or dark voxels.
Edges from $w_e$ and termination energy is a function of curvature.
Curvature can measured 

\begin{equation}
	 E_{int}= \alpha(s)\left|\frac{d\mathbf{v}(s)}{ds}\right|^2 + \beta(s)\left|\frac{d^2\mathbf{v}(s)}{ds^2}\right|^2 
	 \label{eqn:contour_int}
\end{equation}

\begin{equation}
	 E_{ext}= w_{l}\mathbf{I} + w_{e}|\nabla \mathbf{I}|^2 + w_{t}E_c(\mathbf{I})
	 \label{eqn:contour_int}
\end{equation}















\section{Feature Selection}
\label{sec:selection}

Commonly large datasets contain some features in the input space will be have no effect upon or even reduce the performance of a classifier.
It is naive to use just the raw image data.
Tumour features for extraction have been considered in section~\ref{sec:tumour} which already removes some redundant information. 
Mapping input data into a lower dimensional space is possible and can improve performance but if ill-applied can remove key features.
A brute-force appraoch would find the optimal combination of features but the input space is expected to be too large, $2^n$, to be completely shattered within reasonable execution time. 



\subsection{Sequential Floating Search (SFS)}

This heuristic algorithm is applied to tumour classification in~\cite{hau07feat}.
It is possible to start from either the full or empty set of features then iteratively add and remove features to improve the performance of the classifier. 
A performance function is required for set comparison. 
Listing~\ref{lst:sfs} contains the algorithm starting from an initial subset of no features and repeated until convergence.

\lstinputlisting[label=lst:sfs,caption=SFS Algorithm]{Code/sfs.txt}



\subsection{Stochastic Approaches}
A Random Mutation Hill Climber (RMHC) can move in the feature selection space from an initial starting point but will never reduce in performance.  
This does not handle a landscape with many local maxima well and may not produce a reasonable solution.
Genetic algorithms are more computationally expensive as they require a population of solutions using crossover and mutation to converge on a suitable solution.   
Whether it out performs a RMHC depends on the performance landscape but using a population of solutions is certainly more thorough.    
Simulated annealing can be used to move between solution neighbours subject over a fixed number of iterations.
A cooling schedule is used to except negative moves in performance with a probability the is a function of the cooling variable. 


\subsection{Principle Component Analysis (PCA)}
PCA is a common technique used for dimensionality reduction by ranking dimensions in order of variance~\cite{pearson01pca}.
This will rank the features so that only the first $L$ are passed on for further processing. 
The other dimensions can simply be removed but if the effect of a low variance dimension is important to classification this can reduce performance.
Eq.~\eqref{eqn:PCA} shows how the original input pattern is mapped to a reduce pattern by subtracting the mean of all patterns and multiplying with the projection matrix.
The projection matrix is built by finding the covariance matrix of the input patterns then ranking the eigenvectors by the eigenvalues and deleting as required.
Performance feedback from training can be used to select appropriate $L$ value.

\begin{equation}
	\boldsymbol{z} = \boldsymbol{P}(\boldsymbol{x} - \boldsymbol{\mu})
	\label{eqn:PCA}
\end{equation}












\section{Classification}
\label{sec:class}

Classification must differentiate between extracted geometric shapes that are normal and those which may potentially be tumours.
Tumours will also be attached to normal internal body parts so the classifier must be able to handle any abnormalities in organs.
Supervised techniques for classification are considered even though a datasets have not yet been sourced.



\subsection{Artificial Neutral Networks (ANNs)}
ANNs contain summation nodes where data is propagated through weighted connections resulting in a final output. 
As a supervised technique weights are trained by propagating the error backwards through the network.
Multilayer nets can provided non-linear separation functions and increasing the number of layers only increases possible function complexity.




\subsection{Support Vector Machines (SVMs)}
An SVM classifier builds a separation margin supported by the closest data points the drawing a separation plane from the middle of this margin.
A linear maximum-margin hyperplane is produce but non-linear data can also be separated by using the kernel trick~\cite{cortes95support}. 
This is a mapping of the input space into a higher dimensional space so that it becomes linearly separable.
SVMs have been successfully applied to breast cancer diagnosis~\cite{xiufeng13svm}
The approach uses a weighted combination of polynomial and radial basis functions to improve the generalisation performance.



\subsection{Boosting}
A weak learner for binary classification can be considered any classifier with an error rate less than $0.5$.
It is possible to create a strong learner, having a much smaller error rate, by combing the weighted sum of different weak hypothesises provided by weak learners. 
The algorithm can be considered a hybrid of feature selection and classification.

AdaBoost is the most common implementation of a boosting algorithm and has been successfully applied to brain tumour classification~\cite{freund99boost,islam13multi}.
The architecture of AdaBoost can be thought of as single layer ANN but trained using the error in each weak hypothesis containing with an input dimensions equivalent to the number of boosting rounds.
The weight for each hypothesis is calculated using Eq.~\eqref{eqn:adaboost} where $\epsilon_t$ is the error which means anything greater than $0.5$ is negative and as $\epsilon_t \to 0$ then $\alpha_t \to \infty$.
The distribution of data to train the weak learner for the next hypothesis is also determined using Eq.~\eqref{eqn:adaboost} therefore contributing to feature selection.


\begin{equation}
	\alpha_t = \frac{1}{2}\ln\left(\frac{1 - \epsilon_t}{\epsilon_t}\right)
	\label{eqn:adaboost}
\end{equation}






\section{Conclusions}
\label{sec:conclusions}

It is clear that this tool can't be used as a hard and fast method of tumour diagnosis but instead as a valuable tool for image reviewers.
Software packages designed for medical image review, such as~\cite{slicer}, have bidirectional interfaces to custom extensions.
Implementing this system as an extension would remove many overheads required in data handling but reduce the scope for optimisation.
The feedback from the entire process would be a checklist which the user must manually review and waive if incorrectly identified.

Medical images are high resolution but noise can still be issue and a pre-processing stage is reasonable.
The acquisition method should be verified to contain no image processing as this could produce redundancy.
Anisotropic diffusion is recommended to reduce noise because edge detection is important in later stages.
This method can also by easily tuned by limiting iterations of algorithm.

Edge detection is for both pre-processing and feature extraction.
Canny edge detection produces significantly better results than simple filter kernels but the expense of computation.








\bibliographystyle{ieeetran}
\bibliography{references}

\end{document}


